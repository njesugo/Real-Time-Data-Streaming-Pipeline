Dans ce projet, il s’agit de réaliser une pipeline de donnée en streaming, autrement dit une pipeline qui fait des appels constante de données, les traitent et les stockent en temps presque réel. 

Selon cette architecture, nous appelons les données d’une API nommée RappelConso (une api en accès du gouvernement français), ces nous les consommons en utilisant Kafka. Ensuite Nous les traitons avec Spark Streaming pour enfin les stocker dans une base de données PostgreSQL. Les tâches de consommation et traitement sont planifiées et exécutées avec Airflow. Enfin tout le projet est compilé et tourné avec Docker.
